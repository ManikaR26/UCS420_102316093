{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1WScjfMGj9z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Text normalization, tokenization, stopword removal & frequency\n",
        "Below we:\n",
        "\n",
        "Define a 5–6 sentence paragraph on “technology”\n",
        "\n",
        "Convert to lowercase & strip punctuation\n",
        "\n",
        "Tokenize into sentences & words\n",
        "\n",
        "Remove English stopwords\n",
        "Compute and display word frequency (excluding stopwords)\n"
      ],
      "metadata": {
        "id": "262Km2oNGk1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')       # For tokenization\n",
        "nltk.download('stopwords')   # For stopword removal\n",
        "nltk.download('wordnet')     # For lemmatization\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Su5uMB-8GsWU",
        "outputId": "7b15025a-6b05-4143-f336-eec649ea25fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1: setup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# If running first time, you may need:\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# 1. Define paragraph\n",
        "text = \"\"\"\n",
        "Over the past few years, artificial intelligence has dramatically changed how we engage with modern technology.\n",
        "From digital assistants that handle casual conversations to autonomous vehicles maneuvering through traffic, the influence is significant.\n",
        "Programmers are striving to create more intelligent systems that can adapt and learn instantly.\n",
        "At the same time, issues related to data security and the responsible use of AI are becoming increasingly important.\n",
        "It's evident that advancements in machine learning and deep neural networks will define the coming decade.\n",
        "\"\"\"\n",
        "\n",
        "# 2. Lowercase & remove punctuation\n",
        "clean = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# 3. Tokenize\n",
        "sentences = sent_tokenize(clean)\n",
        "words = word_tokenize(clean)\n",
        "\n",
        "# 4. Remove stopwords\n",
        "stops = set(stopwords.words('english'))\n",
        "filtered = [w for w in words if w not in stops and w.isalpha()]\n",
        "\n",
        "# 5. Word frequency\n",
        "freq = Counter(filtered)\n",
        "print(\"Sentence tokens:\", sentences)\n",
        "print(\"Word tokens:\", words)\n",
        "print(\"Filtered words:\", filtered)\n",
        "print(\"Word Frequency Distribution:\")\n",
        "for word, count in freq.most_common():\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IjbhvQJHOI7",
        "outputId": "e53cd057-3ba7-4840-f767-80c308bf8319"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence tokens: ['\\nover the past few years artificial intelligence has dramatically changed how we engage with modern technology\\nfrom digital assistants that handle casual conversations to autonomous vehicles maneuvering through traffic the influence is significant\\nprogrammers are striving to create more intelligent systems that can adapt and learn instantly\\nat the same time issues related to data security and the responsible use of ai are becoming increasingly important\\nits evident that advancements in machine learning and deep neural networks will define the coming decade']\n",
            "Word tokens: ['over', 'the', 'past', 'few', 'years', 'artificial', 'intelligence', 'has', 'dramatically', 'changed', 'how', 'we', 'engage', 'with', 'modern', 'technology', 'from', 'digital', 'assistants', 'that', 'handle', 'casual', 'conversations', 'to', 'autonomous', 'vehicles', 'maneuvering', 'through', 'traffic', 'the', 'influence', 'is', 'significant', 'programmers', 'are', 'striving', 'to', 'create', 'more', 'intelligent', 'systems', 'that', 'can', 'adapt', 'and', 'learn', 'instantly', 'at', 'the', 'same', 'time', 'issues', 'related', 'to', 'data', 'security', 'and', 'the', 'responsible', 'use', 'of', 'ai', 'are', 'becoming', 'increasingly', 'important', 'its', 'evident', 'that', 'advancements', 'in', 'machine', 'learning', 'and', 'deep', 'neural', 'networks', 'will', 'define', 'the', 'coming', 'decade']\n",
            "Filtered words: ['past', 'years', 'artificial', 'intelligence', 'dramatically', 'changed', 'engage', 'modern', 'technology', 'digital', 'assistants', 'handle', 'casual', 'conversations', 'autonomous', 'vehicles', 'maneuvering', 'traffic', 'influence', 'significant', 'programmers', 'striving', 'create', 'intelligent', 'systems', 'adapt', 'learn', 'instantly', 'time', 'issues', 'related', 'data', 'security', 'responsible', 'use', 'ai', 'becoming', 'increasingly', 'important', 'evident', 'advancements', 'machine', 'learning', 'deep', 'neural', 'networks', 'define', 'coming', 'decade']\n",
            "Word Frequency Distribution:\n",
            "past: 1\n",
            "years: 1\n",
            "artificial: 1\n",
            "intelligence: 1\n",
            "dramatically: 1\n",
            "changed: 1\n",
            "engage: 1\n",
            "modern: 1\n",
            "technology: 1\n",
            "digital: 1\n",
            "assistants: 1\n",
            "handle: 1\n",
            "casual: 1\n",
            "conversations: 1\n",
            "autonomous: 1\n",
            "vehicles: 1\n",
            "maneuvering: 1\n",
            "traffic: 1\n",
            "influence: 1\n",
            "significant: 1\n",
            "programmers: 1\n",
            "striving: 1\n",
            "create: 1\n",
            "intelligent: 1\n",
            "systems: 1\n",
            "adapt: 1\n",
            "learn: 1\n",
            "instantly: 1\n",
            "time: 1\n",
            "issues: 1\n",
            "related: 1\n",
            "data: 1\n",
            "security: 1\n",
            "responsible: 1\n",
            "use: 1\n",
            "ai: 1\n",
            "becoming: 1\n",
            "increasingly: 1\n",
            "important: 1\n",
            "evident: 1\n",
            "advancements: 1\n",
            "machine: 1\n",
            "learning: 1\n",
            "deep: 1\n",
            "neural: 1\n",
            "networks: 1\n",
            "define: 1\n",
            "coming: 1\n",
            "decade: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Stemming vs Lemmatization\n",
        "Using the filtered word list from Q1:\n",
        "\n",
        "Porter Stemmer\n",
        "\n",
        "Lancaster Stemmer\n",
        "\n",
        "WordNet Lemmatizer\n",
        "\n",
        "Compare outputs side by side"
      ],
      "metadata": {
        "id": "saozv2lIHwXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2: stemming & lemmatization\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "results = []\n",
        "for w in filtered:\n",
        "    results.append({\n",
        "        'original': w,\n",
        "        'porter': porter.stem(w),\n",
        "        'lancaster': lancaster.stem(w),\n",
        "        'lemma': lemmatizer.lemmatize(w)\n",
        "    })\n",
        "\n",
        "# Display in a neat table\n",
        "import pandas as pd\n",
        "df2 = pd.DataFrame(results)\n",
        "print(df2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJmh3H8mH2aH",
        "outputId": "79313fac-26f0-4150-8eb6-1a6cfa7633ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         original        porter  lancaster         lemma\n",
            "0            past          past       past          past\n",
            "1           years          year       year          year\n",
            "2      artificial      artifici        art    artificial\n",
            "3    intelligence      intellig   intellig  intelligence\n",
            "4    dramatically        dramat       dram  dramatically\n",
            "5         changed         chang      chang       changed\n",
            "6          engage         engag        eng        engage\n",
            "7          modern        modern     modern        modern\n",
            "8      technology     technolog  technolog    technology\n",
            "9         digital         digit      digit       digital\n",
            "10     assistants        assist     assist     assistant\n",
            "11         handle         handl      handl        handle\n",
            "12         casual        casual        cas        casual\n",
            "13  conversations       convers    convers  conversation\n",
            "14     autonomous       autonom    autonom    autonomous\n",
            "15       vehicles        vehicl      vehic       vehicle\n",
            "16    maneuvering        maneuv     maneuv   maneuvering\n",
            "17        traffic       traffic      traff       traffic\n",
            "18      influence      influenc      influ     influence\n",
            "19    significant      signific       sign   significant\n",
            "20    programmers      programm    program    programmer\n",
            "21       striving        strive   striving      striving\n",
            "22         create         creat        cre        create\n",
            "23    intelligent      intellig   intellig   intelligent\n",
            "24        systems        system     system        system\n",
            "25          adapt         adapt      adapt         adapt\n",
            "26          learn         learn      learn         learn\n",
            "27      instantly     instantli       inst     instantly\n",
            "28           time          time        tim          time\n",
            "29         issues          issu       issu         issue\n",
            "30        related         relat        rel       related\n",
            "31           data          data        dat          data\n",
            "32       security         secur        sec      security\n",
            "33    responsible       respons    respons   responsible\n",
            "34            use           use         us           use\n",
            "35             ai            ai         ai            ai\n",
            "36       becoming         becom      becom      becoming\n",
            "37   increasingly  increasingli    increas  increasingly\n",
            "38      important        import     import     important\n",
            "39        evident          evid       evid       evident\n",
            "40   advancements        advanc        adv   advancement\n",
            "41        machine        machin     machin       machine\n",
            "42       learning         learn      learn      learning\n",
            "43           deep          deep       deep          deep\n",
            "44         neural        neural       neur        neural\n",
            "45       networks       network    network       network\n",
            "46         define         defin      defin        define\n",
            "47         coming          come        com        coming\n",
            "48         decade         decad      decad        decade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Regular Expressions & Text Splitting\n",
        "Starting from the original (normalized) text:\n",
        "\n",
        "Extract with regex:\n",
        "\n",
        "Words > 5 letters\n",
        "Numbers (if any)\n",
        "Capitalized words\n",
        "Using split:\n",
        "\n",
        "Words containing only alphabets\n",
        "\n",
        "Words starting with a vowel"
      ],
      "metadata": {
        "id": "D1wRwhuLIBhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3: regex & splitting\n",
        "import re\n",
        "\n",
        "orig = text  # use the original (with punctuation & case)\n",
        "\n",
        "# a. >5 letters\n",
        "long_words = re.findall(r'\\b\\w{6,}\\b', orig)\n",
        "# b. Numbers\n",
        "numbers = re.findall(r'\\d+(?:\\.\\d+)?', orig)\n",
        "# c. Capitalized words\n",
        "capitalized = re.findall(r'\\b[A-Z][a-z]+\\b', orig)\n",
        "\n",
        "# d. Split into alpha-only words\n",
        "alpha_words = re.findall(r'\\b[A-Za-z]+\\b', orig)\n",
        "# e. Words starting with vowel\n",
        "vowel_words = [w for w in alpha_words if re.match(r'^[AEIOUaeiou]', w)]\n",
        "\n",
        "print(\"Words >5 letters:\", long_words)\n",
        "print(\"Numbers:\", numbers)\n",
        "print(\"Capitalized words:\", capitalized)\n",
        "print(\"Alpha-only words:\", alpha_words)\n",
        "print(\"Words starting with vowel:\", vowel_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAbfCQweIGaQ",
        "outputId": "3e0b18e1-5f76-48d4-ded6-e2cc37d32778"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words >5 letters: ['artificial', 'intelligence', 'dramatically', 'changed', 'engage', 'modern', 'technology', 'digital', 'assistants', 'handle', 'casual', 'conversations', 'autonomous', 'vehicles', 'maneuvering', 'through', 'traffic', 'influence', 'significant', 'Programmers', 'striving', 'create', 'intelligent', 'systems', 'instantly', 'issues', 'related', 'security', 'responsible', 'becoming', 'increasingly', 'important', 'evident', 'advancements', 'machine', 'learning', 'neural', 'networks', 'define', 'coming', 'decade']\n",
            "Numbers: []\n",
            "Capitalized words: ['Over', 'From', 'Programmers', 'At', 'It']\n",
            "Alpha-only words: ['Over', 'the', 'past', 'few', 'years', 'artificial', 'intelligence', 'has', 'dramatically', 'changed', 'how', 'we', 'engage', 'with', 'modern', 'technology', 'From', 'digital', 'assistants', 'that', 'handle', 'casual', 'conversations', 'to', 'autonomous', 'vehicles', 'maneuvering', 'through', 'traffic', 'the', 'influence', 'is', 'significant', 'Programmers', 'are', 'striving', 'to', 'create', 'more', 'intelligent', 'systems', 'that', 'can', 'adapt', 'and', 'learn', 'instantly', 'At', 'the', 'same', 'time', 'issues', 'related', 'to', 'data', 'security', 'and', 'the', 'responsible', 'use', 'of', 'AI', 'are', 'becoming', 'increasingly', 'important', 'It', 's', 'evident', 'that', 'advancements', 'in', 'machine', 'learning', 'and', 'deep', 'neural', 'networks', 'will', 'define', 'the', 'coming', 'decade']\n",
            "Words starting with vowel: ['Over', 'artificial', 'intelligence', 'engage', 'assistants', 'autonomous', 'influence', 'is', 'are', 'intelligent', 'adapt', 'and', 'instantly', 'At', 'issues', 'and', 'use', 'of', 'AI', 'are', 'increasingly', 'important', 'It', 'evident', 'advancements', 'in', 'and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Custom Tokenizer & Regex-based Cleaning\n",
        "Write a tokenizer that:\n",
        "\n",
        "Keeps contractions together (isn't)\n",
        "\n",
        "Treats hyphens as part of words (state-of-the-art)\n",
        "\n",
        "Separates numbers (but keeps decimals together)\n",
        "\n",
        "Use re.sub to:\n",
        "\n",
        "Replace emails → <EMAIL>\n",
        "\n",
        "Replace URLs → <URL>\n",
        "\n",
        "Replace phone nos. (123-456-7890 or +91 9876543210) → <PHONE>"
      ],
      "metadata": {
        "id": "w5tzfgTuIMs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4: custom tokenizer & cleaning\n",
        "def custom_tokenize(s):\n",
        "    # placeholder for emails/URLs/phones before tokenization\n",
        "    s = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', s)\n",
        "    s = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', s)\n",
        "    s = re.sub(r'\\+?\\d{1,3}[\\s-]\\d{6,10}', '<PHONE>', s)\n",
        "    s = re.sub(r'\\b\\d+\\.\\d+\\b', lambda m: f\"<NUM:{m.group(0)}>\", s)  # mark decimals\n",
        "    s = re.sub(r'\\b\\d+\\b', '<NUM>', s)\n",
        "    # now split on whitespace & punctuation except hyphens/apostrophes\n",
        "    tokens = re.findall(r\"[A-Za-z0-9<>:_]+(?:['-][A-Za-z0-9]+)*\", s)\n",
        "    # restore decimal tokens\n",
        "    return [t.replace('<NUM:', '').replace('>', '') if t.startswith('<NUM:') else t for t in tokens]\n",
        "\n",
        "sample = text.strip()\n",
        "tokens_q4 = custom_tokenize(sample)\n",
        "print(\"Custom tokens:\", tokens_q4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANGnP8NwIUsT",
        "outputId": "2992a6b1-654a-4b26-c176-e8026458bb29"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom tokens: ['Over', 'the', 'past', 'few', 'years', 'artificial', 'intelligence', 'has', 'dramatically', 'changed', 'how', 'we', 'engage', 'with', 'modern', 'technology', 'From', 'digital', 'assistants', 'that', 'handle', 'casual', 'conversations', 'to', 'autonomous', 'vehicles', 'maneuvering', 'through', 'traffic', 'the', 'influence', 'is', 'significant', 'Programmers', 'are', 'striving', 'to', 'create', 'more', 'intelligent', 'systems', 'that', 'can', 'adapt', 'and', 'learn', 'instantly', 'At', 'the', 'same', 'time', 'issues', 'related', 'to', 'data', 'security', 'and', 'the', 'responsible', 'use', 'of', 'AI', 'are', 'becoming', 'increasingly', 'important', \"It's\", 'evident', 'that', 'advancements', 'in', 'machine', 'learning', 'and', 'deep', 'neural', 'networks', 'will', 'define', 'the', 'coming', 'decade']\n"
          ]
        }
      ]
    }
  ]
}